---
title: "Data Science for Public Policy"
subtitle: "Assignment XX"
author: "Name - NetID"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

```{r}
library(tidyverse)
library(dplyr)
library(tidymodels)
library(patchwork)
library(tidyclust)
library(tidytext)
library(igraph)
library(ggplot2)
library(ggraph)
library(graphlayouts)
library(Rfast)
```


# Exercise 01

```{r}
votes <- read_csv("votes_time_series.csv")

# Replace NA values with 0

votes <- votes %>%
  replace(is.na(.), 0)

# Save votes for session 103
votes_103 <- votes %>%
  filter(session == 103)

# Create a recipe to run principal component analysis

votes_pca_rec <- recipe( ~ ., data = votes_103) %>%
  step_pca(all_numeric_predictors(), id = "pca", num_comp = 5) 
  

# calculate pct variance explained by components

votes_pca_rec %>%
prep() %>%
tidy(id = "pca", type = "variance") %>%
filter(terms == "variance") %>%
mutate(pct_var = value/sum(value)) %>%
slice_head(n = 5)

# The first component explains about 95% of the variance and the second component explains about 2%. The next three explain less than 1% of the variance.

votes_pcs <- votes_pca_rec %>%
  prep() %>%
  bake(new_data = votes_103)

# Creating plots using the PCAs 

party_point <- votes_pcs %>%
  ggplot() +
  geom_point(mapping = aes(x = PC1, y = PC2, color = party))

# Appending votes dataset to include regions

regions <- read_csv("states_regions.csv")

# Visualize votes by political party
votes_pcs <- left_join(votes_pcs, regions, join_by("state" == "State Code"))

# Visualize votes by region
region_point <- votes_pcs %>%
  ggplot() + 
  geom_point(mapping = aes(x = PC1, y = PC2, color = Region))

party_point + region_point
```

# Exercise 02

```{r}
set.seed(20220412) 

kmeans_rec <- recipe(
formula = ~ .,
data = votes_103
) %>%
step_select(all_numeric())

# set up cross-validation
votes_cv <- vfold_cv(votes_103, v = 5)

kmeans_spec <- k_means(
num_clusters = tune()) %>%
set_engine("stats",
nstart = 100) # number of random starts

# create a workflow
kmeans_wflow <- workflow(
preprocessor = kmeans_rec,
spec = kmeans_spec
)
# create tuning grid
clust_num_grid <- grid_regular(
num_clusters(),
levels = 10
)
# see the tuning grid
clust_num_grid

res <- tune_cluster(
kmeans_wflow,
resamples = votes_cv,
grid = clust_num_grid,
control = control_grid(save_pred = TRUE, extract = identity),
metrics = cluster_metric_set(sse_within_total, silhouette_avg))

res_metrics <- res %>%
collect_metrics()

```


```{r}
# Creating the function to run for different numbers of clusters

kmeans_pca_viz <- function(k, df){
  
kmeans_spec <- k_means(
num_clusters = k) %>%
set_engine("stats",
nstart = 100) # number of random starts

# create a workflow
kmeans_wflow <- workflow(
preprocessor = kmeans_rec,
spec = kmeans_spec
)

# fit the model
kmeans_mod <- kmeans_wflow %>%
  fit(data = df)

kmeans_votes <- bind_cols(
  select(df, name, party),
  select(votes_pcs, PC1, PC2),
  cluster2 = kmeans_mod %>%
    extract_cluster_assignment() %>%
    pull(.cluster)
)

ggplot() + 
  geom_point(
    data = kmeans_votes,
    mapping = aes(PC1, PC2, color = factor(cluster2)),
    alpha = 0.5
  ) +
scale_color_manual(values = c("blue", "red")) +
labs(
title = "K-Means with K=2 and PCA",
x = "PC1 (0.95 of Variation)",
y = "PC2 (0.02 of Variation)"
) +
theme_minimal() +
guides(text = NULL)
# Produce a scatterplot with PC1 on the x-axis and PC2 on the y axis with clusters as the points 

}

print(kmeans_pca_viz(2, votes_103))

```
# Exercise 03

```{r}
executive_orders <- read_csv("executive-orders.csv")

exec_orders <- executive_orders[!(is.na(executive_orders$text)), ] # Omit NA rows

exec_orders <- exec_orders %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

exec_orders_separated <- separate(exec_orders, bigram, c("word1", "word2"), sep = " ")

stop_words <- stop_words 

exec_orders_separated <- exec_orders_separated %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word"))

# Count the number of appearances of each bigram
bigram_counts <- exec_orders_separated %>%
  group_by(word1, word2) %>%
  summarise(appearances = n())

# Filter to rows with more than 150 appearances
bigram_150 <- bigram_counts %>%
  filter(appearances > 150)

# plot the bigrams that exist more than 150 times
bigram_graph <- bigram_150 %>%
graph_from_data_frame()

# plot the relationships (you may want to make the plot window bigger)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```


```{r}

bigram_president_count <- separate(exec_orders, bigram, c("word1", "word2"), sep = " ")

# Filter out stopwords
bigram_president_count <- bigram_president_count %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word")) 

# Then rejoin the column to calculate number of bigram-president pairs and tf-idf 
bigram_president_count <- unite(bigram_president_count, col = bigram, c("word1", "word2"), sep = " ")

bigram_president_count <- bigram_president_count %>%    
group_by(president, bigram) %>%
  summarise(appearances = n())

# Calculate TF-IDF
tf_idf <- bigram_president_count %>%
  count(president, bigram, sort = TRUE) %>%
  bind_tf_idf(term = bigram, document = president, n = n)

# plot TF-IDF
tf_idf %>%
group_by(president) %>%
top_n(15, tf_idf) %>%
ggplot(aes(tf_idf, bigram, fill = president)) +
geom_col() +
facet_wrap(~president, scales = "free") +
theme_minimal() +
guides(fill = "none")


```

# Exercise 04
```{r}
# Read csv
bills <- read_csv("senate_bills_114.csv") %>%
mutate(passed = factor(passed, labels = c("1", "0"), levels = c("1", "0")))

set.seed(20220414)

split <- initial_split(data = bills, strata = "passed", prop = 0.8)

bills_train <- training(x = split)
bills_test <- testing(x = split)

library(textrecipes)

library(stopwords)

# Creating a recipe to perform TF-IDF
bills_rec <- recipe( ~ ., data = bills) %>%
  step_tokenize(description) %>%
  step_stopwords(description) %>%
  step_stem(description) %>%
  step_tokenfilter(description, max_tokens = 200) %>%
  step_tfidf(description)


bills_obj <- bills_rec %>%
  prep() 

baked_training <- bills_obj %>%
  bake(new_data = bills_train)

# domain-specific words: bill, congress, author, administration, act

# Recreate recipe to filter out domain-specific stopwords

bills_rec <- recipe( ~ ., data = bills) %>%
  step_tokenize(description) %>%
  step_stopwords(description, custom_stopword_source = c("bill", "congress", "author", "administration", "act")) %>%
  step_stem(description) %>%
  step_tokenfilter(description, max_tokens = 200) %>%
  step_tfidf(description)

```

